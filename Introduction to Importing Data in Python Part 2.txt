1. Introduction and flat files
- Importing flat files from the web: your turn!
    # Import package
    from urllib.request import urlretrieve

    # Import pandas
    import pandas as pd

    # Assign url of file: url
    url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'

    # Save file locally
    urlretrieve(url, 'winequality-red.csv')

    # Read file into a DataFrame and print its head
    df = pd.read_csv('winequality-red.csv', sep=';')
    print(df.head())

- Opening and reading flat files from the web
    # Import packages
    import matplotlib.pyplot as plt
    import pandas as pd

    # Assign url of file: url
    url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'

    # Read file into a DataFrame: df
    df = pd.read_csv(url,sep=';')

    # Print the head of the DataFrame
    print(df.head())

    # Plot first column of df
    pd.DataFrame.hist(df.ix[:, 0:1])
    plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')
    plt.ylabel('count')
    plt.show()

- Importing non-flat files from the web
    # Import package
    import pandas as pd

    # Assign url of file: url
    url = 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'

    # Read in all sheets of Excel file: xls
    xls = pd.read_excel(url, sheet_name = None)

    # Print the sheetnames to the shell
    print(xls.keys())

    # Print the head of the first sheet (using its name, NOT its index)
    print(xls['1700'].head())

- Performing HTTP requests in Python using urllib
    # Import packages
    from urllib.request import urlopen, Request

    # Specify the url
    url = "https://campus.datacamp.com/courses/1606/4135?ex=2"

    # This packages the request: request
    request = Request(url)

    # Sends the request and catches the response: response
    response = urlopen(request)

    # Print the datatype of response
    print(type(response))

    # Be polite and close the response!
    response.close()

- Printing HTTP request results in Python using urllib
    # Import packages
    from urllib.request import urlopen, Request

    # Specify the url
    url = "https://campus.datacamp.com/courses/1606/4135?ex=2"

    # This packages the request
    request = Request(url)

    # Sends the request and catches the response: response
    response = urlopen(request)

    # Extract the response: html
    html = response.read()

    # Print the html
    print(html)

    # Be polite and close the response!
    response.close()

- Performing HTTP requests in Python using requests
    # Import package
    import requests

    # Specify the url: url
    url = "http://www.datacamp.com/teach/documentation"

    # Packages the request, send the request and catch the response: r
    r = requests.get(url)

    # Extract the response: text
    text = r.text

    # Print the html
    print(text)

- Parsing HTML with BeautifulSoup
    # Import packages
    import requests
    from bs4 import BeautifulSoup

    # Specify url: url
    url = 'https://www.python.org/~guido/'

    # Package the request, send the request and catch the response: r
    r = requests.get(url)

    # Extracts the response as html: html_doc
    html_doc = r.text

    # Create a BeautifulSoup object from the HTML: soup
    soup = BeautifulSoup(html_doc)

    # Prettify the BeautifulSoup object: pretty_soup
    pretty_soup = soup.prettify()

    # Print the response
    print(pretty_soup)

- Turning a webpage into data using BeautifulSoup: getting the text
    # Import packages
    import requests
    from bs4 import BeautifulSoup

    # Specify url: url
    url = 'https://www.python.org/~guido/'

    # Package the request, send the request and catch the response: r
    r = requests.get(url)

    # Extract the response as html: html_doc
    html_doc = r.text

    # Create a BeautifulSoup object from the HTML: soup
    soup = BeautifulSoup(html_doc)

    # Get the title of Guido's webpage: guido_title
    guido_title = soup.title

    # Print the title of Guido's webpage to the shell
    print(guido_title)

    # Get Guido's text: guido_text
    guido_text = soup.get_text()

    # Print Guido's text to the shell
    print(guido_text)

- Turning a webpage into data using BeautifulSoup: getting the hyperlinks
    # Import packages
    import requests
    from bs4 import BeautifulSoup

    # Specify url
    url = 'https://www.python.org/~guido/'

    # Package the request, send the request and catch the response: r
    r = requests.get(url)

    # Extracts the response as html: html_doc
    html_doc = r.text

    # create a BeautifulSoup object from the HTML: soup
    soup = BeautifulSoup(html_doc)

    # Print the title of Guido's webpage
    print(soup.title)

    # Find all 'a' tags (which define hyperlinks): a_tags
    a_tags = soup.find_all('a')

    # Print the URLs to the shell
    for link in a_tags:
        print(link.get('href'))


2. Intermediate Importing Data in Python
- Loading and exploring a JSON
    # Load JSON: json_data
    with open("a_movie.json") as json_file:
        json_data = json.load(json_file)

    # Print each key-value pair in json_data
    for k in json_data.keys():
        print(k + ': ', json_data[k])

- API requests
    # Import requests package
    import requests

    # Assign URL to variable: url
    url = 'http://www.omdbapi.com/?apikey=72bc447a&t=the+social+network'

    # Package the request, send the request and catch the response: r
    r = requests.get(url)

    # Print the text of the response
    print(r.text)

- JSONâ€“from the web to Python
    # Import package
    import requests

    # Assign URL to variable: url
    url = 'http://www.omdbapi.com/?apikey=72bc447a&t=social+network'

    # Package the request, send the request and catch the response: r
    r = requests.get(url)

    # Decode the JSON data into a dictionary: json_data
    json_data = r.json()

    # Print each key-value pair in json_data
    for k in json_data.keys():
        print(k + ': ', json_data[k])

- Checking out the Wikipedia API
    # Import package
    import requests

    # Assign URL to variable: url
    url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'

    # Package the request, send the request and catch the response: r
    r = requests.get(url)

    # Decode the JSON data into a dictionary: json_data
    json_data = r.json()

    # Print the Wikipedia page extract
    pizza_extract = json_data['query']['pages']['24768']['extract']
    print(pizza_extract)

3. Diving deep into the Twitter API
- API Authentication
    # Import package
    import tweepy

    # Store OAuth authentication credentials in relevant variables
    access_token = "1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy"
    access_token_secret = "X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx"
    consumer_key = "nZ6EA0FxZ293SxGNg8g8aP0HM"
    consumer_secret = "fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i"

    # Pass OAuth details to tweepy's OAuth handler
    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_token, access_token_secret)

- Streaming tweets
    # Initialize Stream listener
    l = MyStreamListener()

    # Create your Stream object with authentication
    stream = tweepy.Stream(auth, l)

    # Filter Twitter Streams to capture data by the keywords:
    stream.filter(track = ['clinton', 'trump', 'sanders', 'cruz'])

- Load and explore your Twitter data
    # Import package
    import json

    # String of path to file: tweets_data_path
    tweets_data_path = 'tweets.txt'

    # Initialize empty list to store tweets: tweets_data
    tweets_data = []

    # Open connection to file
    tweets_file = open(tweets_data_path, "r")

    # Read in tweets and store in list: tweets_data
    for line in tweets_file:
        tweet = json.loads(line)
        tweets_data.append(tweet)

    # Close connection to file
    tweets_file.close()

    # Print the keys of the first tweet dict
    print(tweets_data[0].keys())

- Twitter data to DataFrame
    # Import package
    import pandas as pd

    # Build DataFrame of tweet texts and languages
    df = pd.DataFrame(tweets_data, columns=['text','lang'])

    # Print head of DataFrame
    print(df.head())

- A little bit of Twitter text analysis
    # Initialize list to store tweet counts
    [clinton, trump, sanders, cruz] = [0, 0, 0, 0]

    # Iterate through df, counting the number of tweets in which
    # each candidate is mentioned
    for index, row in df.iterrows():
        clinton += word_in_text('clinton', row['text'])
        trump += word_in_text('trump', row['text'])
        sanders += word_in_text('sanders', row['text'])
        cruz += word_in_text('cruz', row['text'])

- Plotting your Twitter data
    # Import packages
    import matplotlib.pyplot as plt
    import seaborn as sns

    # Set seaborn style
    sns.set(color_codes=True)

    # Create a list of labels:cd
    cd = ['clinton', 'trump', 'sanders', 'cruz']

    # Plot the bar chart
    ax = sns.barplot(cd, [clinton, trump, sanders, cruz])
    ax.set(ylabel="count")
    plt.show()

3. Advanced data problems
- Uniform currencies
    # Find values of acct_cur that are equal to 'euro'
    acct_eu = banking['acct_cur'] == 'euro'

    # Convert acct_amount where it is in euro to dollars
    banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1 

    # Unify acct_cur column by changing 'euro' values to 'dollar'
    banking.loc[acct_eu, 'acct_cur'] = 'dollar'

    # Assert that only dollar currency remains
    assert banking['acct_cur'].unique() == 'dollar'

- Uniform dates
    # Print the header of account_opend
    print(banking['account_opened'].head())

    # Convert account_opened to datetime
    banking['account_opened'] = pd.to_datetime(banking['account_opened'],
                                               # Infer datetime format
                                               infer_datetime_format = True,
                                               # Return missing value for error
                                               errors = 'coerce') 

    # Get year of account opened
    banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')

    # Print acct_year
    print(banking['acct_year'])

- How's our data integrity?
    # Store fund columns to sum against
    fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']

    # Find rows where fund_columns row sum == inv_amount
    inv_equ = banking[['fund_A', 'fund_B', 'fund_C', 'fund_D']].sum(axis=1) == banking['inv_amount']

    # Store consistent and inconsistent data
    consistent_inv = banking[inv_equ]
    inconsistent_inv = banking[~inv_equ]

    # Store consistent and inconsistent data
    print("Number of inconsistent investments: ", inconsistent_inv.shape[0])
    
- How's our data integrity?
    # Store today's date and find ages
    today = dt.date.today()
    ages_manual = today.year - banking['birth_date'].dt.year

    # Find rows where age column == ages_manual
    age_equ = ages_manual == banking['age']

    # Store consistent and inconsistent data
    consistent_ages = banking[age_equ]
    inconsistent_ages = banking[~age_equ]

    # Store consistent and inconsistent data
    print("Number of inconsistent ages: ", inconsistent_ages.shape[0])

- Missing investors
    # Print number of missing values in banking
    print(banking.isna().sum())

    # Visualize missingness matrix
    msno.matrix(banking)
    plt.show()

    # Isolate missing and non missing values of inv_amount
    missing_investors = banking[banking['inv_amount'].isna()]
    investors = banking[~banking['inv_amount'].isna()]
    print(missing_investors.describe())
    print(investors.describe())

    # Sort banking by age and visualize
    banking_sorted = banking.sort_values(by = 'age')
    msno.matrix(banking_sorted)
    plt.show()

- Follow the money
    # Drop missing values of cust_id
    banking_fullid = banking.dropna(subset = ['cust_id'])

    # Compute estimated acct_amount
    acct_imp = banking_fullid['inv_amount'] * 5

    # Impute missing acct_amount with corresponding acct_imp
    banking_imputed = banking_fullid.fillna({'acct_amount':acct_imp})

    # Print number of missing values
    print(banking_imputed.isna().sum())

4. Record linkage
- The cutoff point
    # Import process from fuzzywuzzy
    from fuzzywuzzy import process
    # Store the unique values of cuisine_type in unique_types
    unique_types = restaurants['cuisine_type'].unique()

    # Calculate similarity of 'asian' to all values of unique_types
    print(process.extract('asian', unique_types, limit = len(unique_types)))

    # Calculate similarity of 'american' to all values of unique_types
    print(process.extract('american', unique_types, limit = len(unique_types)))

    # Calculate similarity of 'italian' to all values of unique_types
    print(process.extract('italian', unique_types, limit = len(unique_types)))
    # 80 is the cutoff point
  
  
- Remapping categories II
    # Inspect the unique values of the cuisine_type column
    print(restaurants['cuisine_type'].unique())

    # Create a list of matches, comparing 'italian' with the cuisine_type column
    matches = process.extract('italian', restaurants['cuisine_type'], limit = len(restaurants['cuisine_type']))

    # Inspect the first 5 matches
    print(matches[0:5])

    # Iterate through the list of matches to italian
    for match in matches:
      # Check whether the similarity score is greater than or equal to 80
      if match[1] >= 80:
        # Select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine
        restaurants.loc[restaurants['cuisine_type'] == match[0], 'cuisine_type'] = 'italian'

    # Iterate through categories
    for cuisine in categories:  
      # Create a list of matches, comparing cuisine with the cuisine_type column
      matches = process.extract(cuisine, restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))

      # Iterate through the list of matches
      for match in matches:
         # Check whether the similarity score is greater than or equal to 80
        if match[1] >= 80:
          # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine
          restaurants.loc[restaurants['cuisine_type'] == match[0]] = cuisine

    # Inspect the final result
    print(restaurants['cuisine_type'].unique())

- Pairs of restaurants
    # Create an indexer and object and find possible pairs
    indexer = recordlinkage.Index()

    # Block pairing on cuisine_type
    indexer.block('cuisine_type')

    # Generate pairs
    pairs = indexer.index(restaurants, restaurants_new)

- Similar restaurants
    # Create a comparison object
    comp_cl = recordlinkage.Compare()

    # Find exact matches on city, cuisine_types 
    comp_cl.exact('city', 'city', label='city')
    comp_cl.exact('cuisine_type', 'cuisine_type', label = 'cuisine_type')

    # Find similar matches of rest_name
    comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8) 

    # Get potential matches and print
    potential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)
    print(potential_matches)

- Linking them together!
    # Isolate potential matches with row sum >=3
    matches = potential_matches[potential_matches.sum(axis = 1) >= 3]

    # Get values of second column index of matches
    matching_indices = matches.index.get_level_values(1)

    # Subset restaurants_new based on non-duplicate values
    non_dup = restaurants_new[~restaurants_new.index.isin(matching_indices)]

    # Append non_dup to restaurants
    full_restaurants = restaurants.append(non_dup)
    print(full_restaurants)

































































